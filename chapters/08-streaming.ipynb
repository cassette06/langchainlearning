{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9sciiStEX3b"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/08-streaming.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye774qInP4o3"
      },
      "source": [
        "#### LangChain Essentials Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtsjqmEUP4o4"
      },
      "source": [
        "# Streaming With Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMSrFSXyP4o4"
      },
      "source": [
        "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
        "\n",
        "In this example, we will introduce LangChain's async streaming, allowing us to receive and view the tokens as they are generated by OpenAI's LLM. The use of streaming is typical in conversational interfaces and can provide a more natural experience for users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS-V4uqsP5nU"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core==0.3.33 \\\n",
        "  langchain-openai==0.3.3 \\\n",
        "  langchain-community==0.3.16 \\\n",
        "  langsmith==0.3.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx2DC8OuP4o5"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCIXuavOP4o5"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdZXycP7P4o5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-streaming-openai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmJY_lLkP4o5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC8CU_GfP4o6"
      },
      "source": [
        "For the LLM, we'll start by initializing our connection to the OpenAI API. We do need an OpenAI API key, which you can get from the [OpenAI platform](https://platform.openai.com/api-keys).\n",
        "\n",
        "We will use the `gpt-4o-mini` model with a `temperature` of `0.0`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHdcekEFP4o6",
        "outputId": "0d7a7e8c-783d-465c-da9f-b7ce9ebb7b08"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
        "    or getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=0.0,\n",
        "    streaming=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZveEYgIP4o6",
        "outputId": "b161d7d8-1274-44d2-d47b-f7ad4f2a880a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, id='run-4bce7f92-04dc-44f6-b2ee-3d5012e998c4-0')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_out = llm.invoke(\"Hello there\")\n",
        "llm_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "➤ async 是 Python 中声明“异步函数/行为”的关键字\n",
        "你可以用它做两类事：\n",
        "\n",
        "* 等待某个耗时操作完成（如网络请求） 👉 使用 await\n",
        "\n",
        "* 处理一连串异步结果（比如流式输出） 👉 使用 async for\n",
        "\n",
        "\n",
        "\n",
        " ##### 异步等待\n",
        "👉你是“等着一个任务做完”，然后再处理结果，这叫 异步等待。\n",
        "```py\n",
        "async def get_data():\n",
        "    response = await fetch_from_api()\n",
        "    print(response)\n",
        "```\n",
        "\n",
        "##### 流式输出\n",
        "👉 你是“一边生成，一边处理”，这叫 异步迭代（异步生成器）。\n",
        "```py\n",
        "async for token in stream_tokens():\n",
        "    print(token)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "| 用法    | 关键词         | 场景             | 类比            |\n",
        "| ----- | ----------- | -------------- | ------------- |\n",
        "| 等待结果  | `await`     | 异步调用一个函数       | 点外卖：等送到吃      |\n",
        "| 边来边处理 | `async for` | 异步地遍历一个生成器/数据流 | 直播送礼物：来一个处理一个 |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPFk5sM8P4o6"
      },
      "source": [
        "## Streaming with `astream`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TRMvz7VP4o6"
      },
      "source": [
        "We will start by creating a aysnc stream from our LLM. We do this within an `async for` loop, allowing us to iterate through the chunks of data and use them as soon as the async `astream` method returns the tokens to us. By adding a pipe character `|` we can see the individual tokens that are generated. We set `flush` equal to `True` as this forces immediate output to the console, resulting in smoother streaming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "astream 方法 \n",
        "\n",
        "* 异步流式方法\n",
        "* 返回一个异步迭代器\n",
        "* 每生成一个 token 就立即返回，不等待完整响应"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| 名称    | 本质                                         | 是否用于 `async for` |\n",
        "| ----- | ------------------------------------------ | ---------------- |\n",
        "| 异步生成器 | 使用 `async def + yield` 写成的函数               | ✅ 是，推荐方式         |\n",
        "| 异步迭代器 | 任意一个实现 `__aiter__()` 和 `__anext__()` 方法的对象 | ✅ 是，底层机制         |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZjsC9uUP4o6",
        "outputId": "88c7d86c-cc7c-4f97-8811-0f33a0d81794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|N|LP| stands| for| Natural| Language| Processing|,| which| is| a| sub|field| of| artificial| intelligence| (|AI|)| and| computational| lingu|istics| focused| on| the| interaction| between| computers| and| human| (|natural|)| languages|.| The| goal| of| NLP| is| to| enable| machines| to| understand|,| interpret|,| generate|,| and| respond| to| human| language| in| a| way| that| is| both| meaningful| and| useful|.\n",
            "\n",
            "|N|LP| encompasses| a| variety| of| tasks| and| applications|,| including|:\n",
            "\n",
            "|1|.| **|Text| Analysis|**|:| Understanding| and| extracting| information| from| text|,| such| as| sentiment| analysis|,| topic| modeling|,| and| named| entity| recognition|.\n",
            "\n",
            "|2|.| **|Machine| Translation|**|:| Automatically| translating| text| from| one| language| to| another|,| as| seen| in| tools| like| Google| Translate|.\n",
            "\n",
            "|3|.| **|Speech| Recognition|**|:| Con|verting| spoken| language| into| text|,| which| is| used| in| applications| like| virtual| assistants| (|e|.g|.,| Siri|,| Alexa|).\n",
            "\n",
            "|4|.| **|Text| Generation|**|:| Creating| coherent| and| context|ually| relevant| text| based| on| input| prompts|,| as| seen| in| chat|bots| and| content| generation| tools|.\n",
            "\n",
            "|5|.| **|Question| Answer|ing|**|:| Developing| systems| that| can| answer| questions| posed| in| natural| language|,| often| using| large| databases| or| knowledge| graphs|.\n",
            "\n",
            "|6|.| **|Information| Retrieval|**|:| Enh|ancing| search| engines| to| better| understand| user| queries| and| retrieve| relevant| information|.\n",
            "\n",
            "|N|LP| combines| techniques| from| lingu|istics|,| computer| science|,| and| machine| learning| to| process| and| analyze| large| amounts| of| natural| language| data|.| It| has| numerous| applications| across| various| industries|,| including| customer| service|,| healthcare|,| finance|,| and| more|.||"
          ]
        }
      ],
      "source": [
        "tokens = []\n",
        "async for token in llm.astream(\"What is NLP?\"):\n",
        "    tokens.append(token)\n",
        "    print(token.content, end=\"|\", flush=True)\n",
        "    # flush=True 强制立即输出到控制台，不等待缓冲区满或程序结束"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8lAdYaIP4o6"
      },
      "source": [
        "Since we appended each token to the `tokens` list, we can also see what is inside each and every token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GAzSRmzP4o7",
        "outputId": "4417c755-b843-425e-c0c7-d52170e0b677"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-e0014ed1-ad9a-4b8a-8df4-df2a3e28b45d')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHj3Iy1FP4o7",
        "outputId": "7970092d-4c05-450f-d608-507c9d704359"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessageChunk(content='N', additional_kwargs={}, response_metadata={}, id='run-aea9d2a8-6b81-4d1e-aebc-d2cc59ec5ae3')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPasT2EMP4o7"
      },
      "source": [
        "We can also merge multiple `AIMessageChunk` objects together with the `+` operator, creating a larger set of tokens / chunk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXO4Fk2CP4o7",
        "outputId": "0ee486da-8748-462c-ff2c-5ee3f5fac5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessageChunk(content='NLP', additional_kwargs={}, response_metadata={}, id='run-aea9d2a8-6b81-4d1e-aebc-d2cc59ec5ae3')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens[0] + tokens[1] + tokens[2] + tokens[3] + tokens[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDgHe0V-P4o7"
      },
      "source": [
        "A word of caution, there is nothing preventing you from merging tokens in the incorrect order, so be cautious to not output any token omelettes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpwg02OHP4o7",
        "outputId": "e9647557-997c-4d7a-87f9-b71cb91bbe3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessageChunk(content=' for standsLPN', additional_kwargs={}, response_metadata={}, id='run-aea9d2a8-6b81-4d1e-aebc-d2cc59ec5ae3')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens[4] + tokens[3] + tokens[2] + tokens[1] + tokens[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPBj3ryzP4o7"
      },
      "source": [
        "## Streaming with Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdXrkOkQP4o7"
      },
      "source": [
        "Streaming with agents, particularly the custom agent executor, is a little more complex. Let's begin by constructor a simple agent executor matching what we built in the [Agent Executor](https://github.com/aurelio-labs/langchain-course/blob/main/notebooks/openai/07-custom-agent-executor.ipynb) chapter.\n",
        "\n",
        "To construct the agent executor we need:\n",
        "\n",
        "* Tools\n",
        "* `ChatPromptTemplate`\n",
        "* Our LLM (already defined with `llm`)\n",
        "* An agent\n",
        "* Finally, the agent executor\n",
        "\n",
        "Let's start defining each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b97_81lTP4o7"
      },
      "source": [
        "### Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0T2lL7ZP4o7"
      },
      "source": [
        "Now we will define a few tools to be used by an async agent executor. Our goal for tool-use in regards to streaming are:\n",
        "\n",
        "* The tool-use steps will be streamed in one big chunk, ie we do not return the tool use information token-by-token but instead it streams message-by-message.\n",
        "\n",
        "* The final LLM output _will_ be streamed token-by-token as we saw above.\n",
        "\n",
        "For these we need to define a few math tools and our final answer tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这段话描述了一个混合流式策略:\n",
        "\n",
        "* 工具操作： 按消息块流式输出(结构化信息)\n",
        "\n",
        "* 文本生成： 按token流式输出(自然语言)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w0XZOB6GP4o7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def add(x: float, y: float) -> float:\n",
        "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "@tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
        "    return x * y\n",
        "\n",
        "@tool\n",
        "def exponentiate(x: float, y: float) -> float:\n",
        "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
        "    return x ** y\n",
        "\n",
        "@tool\n",
        "def subtract(x: float, y: float) -> float:\n",
        "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
        "    return y - x\n",
        "\n",
        "@tool\n",
        "def final_answer(answer: str, tools_used: list[str]) -> str:\n",
        "    \"\"\"Use this tool to provide a final answer to the user.\n",
        "    The answer should be in natural language as this will be provided\n",
        "    to the user directly. The tools_used must include a list of tool\n",
        "    names that were used within the `scratchpad`. You MUST use this tool\n",
        "    to conclude the interaction.\n",
        "    \"\"\"\n",
        "    return {\"answer\": answer, \"tools_used\": tools_used}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmmwW965P4o7"
      },
      "source": [
        "We'll need all of our tools in a list when defining our `agent` and `agent_executor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LunxfvJyP4o7"
      },
      "outputs": [],
      "source": [
        "tools = [add, multiply, exponentiate, subtract, final_answer]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdsscIkrP4o7"
      },
      "source": [
        "### `ChatPromptTemplate`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zT6pWCKP4o7"
      },
      "source": [
        "We will create our `ChatPromptTemplate`, using a system message, chat history, user input, and a scratchpad for intermediate steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QEDeZUgdP4o7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"You're a helpful assistant. When answering a user's question \"\n",
        "        \"you should first use one of the tools provided. After using a \"\n",
        "        \"tool the tool output will be provided back to you. You MUST \"\n",
        "        \"then use the final_answer tool to provide a final answer to the user. \"\n",
        "        \"DO NOT use the same tool more than once.\"\n",
        "    )),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qQsb_BeP4o8"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e725jAnP4o8"
      },
      "source": [
        "As before, we will define our `agent` with LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TdCPU1eFP4o8"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables.base import RunnableSerializable\n",
        "\n",
        "tools = [add, subtract, multiply, exponentiate, final_answer]\n",
        "\n",
        "# define the agent runnable\n",
        "agent: RunnableSerializable = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "    }\n",
        "    | prompt\n",
        "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC9a4CwOP4o8"
      },
      "source": [
        "### Agent Executor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcohW7uHP4o8"
      },
      "source": [
        "Finally, we will create the agent executor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "S0lx-l-xP4o8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "\n",
        "\n",
        "# create tool name to function mapping\n",
        "name2tool = {tool.name: tool.func for tool in tools}\n",
        "\n",
        "class CustomAgentExecutor:\n",
        "    chat_history: list[BaseMessage]\n",
        "\n",
        "    def __init__(self, max_iterations: int = 3):\n",
        "        self.chat_history = []\n",
        "        self.max_iterations = max_iterations\n",
        "        self.agent: RunnableSerializable = (\n",
        "            {\n",
        "                \"input\": lambda x: x[\"input\"],\n",
        "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "            }\n",
        "            | prompt\n",
        "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
        "        )\n",
        "\n",
        "    def invoke(self, input: str) -> dict:\n",
        "        # invoke the agent but we do this iteratively in a loop until\n",
        "        # reaching a final answer\n",
        "        count = 0\n",
        "        agent_scratchpad = []\n",
        "        while count < self.max_iterations:\n",
        "            # invoke a step for the agent to generate a tool call\n",
        "            out = self.agent.invoke({\n",
        "                \"input\": input,\n",
        "                \"chat_history\": self.chat_history,\n",
        "                \"agent_scratchpad\": agent_scratchpad\n",
        "            })\n",
        "            # if the tool call is the final answer tool, we stop\n",
        "            if out.tool_calls[0][\"name\"] == \"final_answer\":\n",
        "                break\n",
        "            agent_scratchpad.append(out)  # add tool call to scratchpad\n",
        "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
        "            tool_out = name2tool[out.tool_calls[0][\"name\"]](**out.tool_calls[0][\"args\"])\n",
        "            # add the tool output to the agent scratchpad\n",
        "            action_str = f\"The {out.tool_calls[0]['name']} tool returned {tool_out}\"\n",
        "            agent_scratchpad.append({\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": action_str,\n",
        "                \"tool_call_id\": out.tool_calls[0][\"id\"]\n",
        "            })\n",
        "            print(agent_scratchpad)\n",
        "            # add a print so we can see intermediate steps\n",
        "            print(f\"{count}: {action_str}\")\n",
        "            count += 1\n",
        "        # add the final output to the chat history\n",
        "        final_answer = out.tool_calls[0][\"args\"]\n",
        "        # this is a dictionary, so we convert it to a string for compatibility with\n",
        "        # the chat history\n",
        "        final_answer_str = json.dumps(final_answer)\n",
        "        self.chat_history.append({\"input\": input, \"output\": final_answer_str})\n",
        "        self.chat_history.extend([\n",
        "            HumanMessage(content=input),\n",
        "            AIMessage(content=final_answer_str)\n",
        "        ])\n",
        "        # return the final answer in dict form\n",
        "        return final_answer\n",
        "\n",
        "agent_executor = CustomAgentExecutor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K43Wkw2PP4o8"
      },
      "source": [
        "Our `agent_executor` is now ready to use, let's quickly test it before adding streaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNOL0o_yP4o8",
        "outputId": "00f5c4e9-d3d9-4378-d05f-b09277240484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_IFEM8lMS6bqIuQWYOAKGxKLP', 'function': {'arguments': '{\"x\":10,\"y\":10}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, id='run-fbec83a9-e3c3-46e7-afd2-47f0f50511d6-0', tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': 'call_IFEM8lMS6bqIuQWYOAKGxKLP', 'type': 'tool_call'}]), {'role': 'tool', 'content': 'The add tool returned 20', 'tool_call_id': 'call_IFEM8lMS6bqIuQWYOAKGxKLP'}]\n",
            "0: The add tool returned 20\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'answer': '10 + 10 equals 20.', 'tools_used': ['functions.add']}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke(input=\"What is 10 + 10\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdifQE5SP4o8"
      },
      "source": [
        "Let's modify our `agent_executor` to use streaming and parse the streamed output into a format that we can more easily work with.\n",
        "\n",
        "First, when streaming with our custom agent executor we will need to pass our callback handler to the agent on every new invocation. To make this simpler we can make the `callbacks` field a configurable field and this will allow us to initialize the agent using the `with_config` method, allowing us to pass the callback handler to the agent with every invocation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lOZDrXCuP4o9"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=0.0,\n",
        "    streaming=True\n",
        ").configurable_fields(\n",
        "    # 我会在运行时传入一个“回调处理器”，来处理这些流式生成的 token，也就是说：\n",
        "    # 📦 你把流式输出和回调处理解耦了 —— 流是默认开了的，但具体怎么处理这个流（\n",
        "    # 比如实时打印、推送到前端、存入数据库）由后面传入的 callback handler 决定。\n",
        "    callbacks=ConfigurableField(\n",
        "        id=\"callbacks\",\n",
        "        name=\"callbacks\",\n",
        "        description=\"A list of callbacks to use for streaming\",\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7k1thVGP4o9"
      },
      "source": [
        "We reinitialize our `agent`, nothing changes here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UFVoYB6OP4o9"
      },
      "outputs": [],
      "source": [
        "# define the agent runnable\n",
        "agent: RunnableSerializable = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "    }\n",
        "    | prompt\n",
        "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztlchX8dP4o9"
      },
      "source": [
        "Now, we will define our _custom_ callback handler. This will be a queue callback handler that will allow us to stream the output of the agent through an `asyncio.Queue` object and yield the tokens as they are generated elsewhere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xPpdIQxyP4o9"
      },
      "outputs": [],
      "source": [
        "import asyncio #Python 标准库中用于异步任务调度的模块\n",
        "from langchain.callbacks.base import AsyncCallbackHandler #是 LangChain 提供的异步回调基类，必须继承它才能接收流式 token\n",
        "\n",
        "\n",
        "class QueueCallbackHandler(AsyncCallbackHandler):\n",
        "    \"\"\"Callback handler that puts tokens into a queue.\"\"\"\n",
        "\n",
        "    def __init__(self, queue: asyncio.Queue):\n",
        "        self.queue = queue\n",
        "        self.final_answer_seen = False\n",
        "\n",
        "    async def __aiter__(self): # 生成器函数， 这个方法被async for 自动调用\n",
        "        while True:\n",
        "            if self.queue.empty():\n",
        "                await asyncio.sleep(0.1)\n",
        "                continue\n",
        "            token_or_done = await self.queue.get()\n",
        "\n",
        "            if token_or_done == \"<<DONE>>\":\n",
        "                # this means we're done\n",
        "                return\n",
        "            if token_or_done:\n",
        "                yield token_or_done # <- 在这里暂停，等待调用者处理\n",
        "                # 当调用者处理完毕， 会从这里继续执行\n",
        "                # 然后进入下一次while循环\n",
        "\n",
        "    # 每当llm生成一个token， langchain就会自动调用这个方法，这是一个回调函数\n",
        "    async def on_llm_new_token(self, *args, **kwargs) -> None:\n",
        "        \"\"\"Put new token in the queue.\"\"\"\n",
        "        #print(f\"on_llm_new_token: {args}, {kwargs}\")\n",
        "        chunk = kwargs.get(\"chunk\") # chunk是包含新生成的token和相关信息的对象\n",
        "        if chunk:\n",
        "            # check for final_answer tool call\n",
        "            if tool_calls := chunk.message.additional_kwargs.get(\"tool_calls\"):\n",
        "                if tool_calls[0][\"function\"][\"name\"] == \"final_answer\":\n",
        "                    # this will allow the stream to end on the next `on_llm_end` call\n",
        "                    self.final_answer_seen = True\n",
        "        self.queue.put_nowait(kwargs.get(\"chunk\")) \n",
        "        # put_nowait表示不等待，立即放入队列， 不会阻塞当前协程，适合实时场景，\n",
        "        # 如果队列满了则立即失败，抛出异常。 而await put() 则是等待直到有空间然后成功\n",
        "        return\n",
        "\n",
        "    async def on_llm_end(self, *args, **kwargs) -> None:\n",
        "        \"\"\"Put None in the queue to signal completion.\"\"\"\n",
        "        #print(f\"on_llm_end: {args}, {kwargs}\")\n",
        "        # this should only be used at the end of our agent execution, however LangChain\n",
        "        # will call this at the end of every tool call, not just the final tool call\n",
        "        # so we must only send the \"done\" signal if we have already seen the final_answer\n",
        "        # tool call\n",
        "        if self.final_answer_seen:\n",
        "            self.queue.put_nowait(\"<<DONE>>\")\n",
        "        else:\n",
        "            self.queue.put_nowait(\"<<STEP_END>>\")\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsjISrlRP4o9"
      },
      "source": [
        "We can see how this works together in our `agent` invocation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADNwPcaVP4o9",
        "outputId": "f71e5078-f462-489a-dad5-c256dca424bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_EtZELSAEmKqtAOFXMfwgVIGR', 'function': {'arguments': '', 'name': 'add'}, 'type': 'function'}]} response_metadata={} id='run-5375c74f-192a-4cae-b772-b2440a4af95e' tool_calls=[{'name': 'add', 'args': {}, 'id': 'call_EtZELSAEmKqtAOFXMfwgVIGR', 'type': 'tool_call'}] tool_call_chunks=[{'name': 'add', 'args': '', 'id': 'call_EtZELSAEmKqtAOFXMfwgVIGR', 'index': 0, 'type': 'tool_call_chunk'}]\n",
            "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '{\"', 'name': None}, 'type': None}]} response_metadata={} id='run-5375c74f-192a-4cae-b772-b2440a4af95e' tool_calls=[{'name': '', 'args': {}, 'id': None, 'type': 'tool_call'}] tool_call_chunks=[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
            "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'x', 'name': None}, 'type': None}]} response_metadata={} id='run-5375c74f-192a-4cae-b772-b2440a4af95e' invalid_tool_calls=[{'name': None, 'args': 'x', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': 'x', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
            "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]} response_metadata={} id='run-5375c74f-192a-4cae-b772-b2440a4af95e' invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
            "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '10', 'name': None}, 'type': None}]} response_metadata={} id='run-5375c74f-192a-4cae-b772-b2440a4af95e' invalid_tool_calls=[{'name': None, 'args': '10', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '10', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
            "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ',\"', 'name': None}, 'type': None}]} response_metadata={} id='run-5375c74f-192a-4cae-b772-b2440a4af95e' invalid_tool_calls=[{'name': None, 'args': ',\"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': ',\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
            "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'y', 'name': None}, 'type': None}]} response_metadata={} id='run-5375c74f-192a-4cae-b772-b2440a4af95e' invalid_tool_calls=[{'name': None, 'args': 'y', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': 'y', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
            "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]} response_metadata={} id='run-5375c74f-192a-4cae-b772-b2440a4af95e' invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
            "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '10', 'name': None}, 'type': None}]} response_metadata={} id='run-5375c74f-192a-4cae-b772-b2440a4af95e' invalid_tool_calls=[{'name': None, 'args': '10', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '10', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
            "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '}', 'name': None}, 'type': None}]} response_metadata={} id='run-5375c74f-192a-4cae-b772-b2440a4af95e' invalid_tool_calls=[{'name': None, 'args': '}', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
            "content='' additional_kwargs={} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'} id='run-5375c74f-192a-4cae-b772-b2440a4af95e'\n"
          ]
        }
      ],
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "tokens = []\n",
        "\n",
        "async def stream(query: str):\n",
        "    response = agent.with_config(\n",
        "        callbacks=[streamer] #这里的callbacks对应上述的id = callbacks\n",
        "    )\n",
        "    # 这里的async for并没有触发__aiter__（只会在async for token in streamer中触发）\n",
        "    # llm生成token时，同时触发\n",
        "    # 1.on_llm_new_token()回调(放入队列)\n",
        "    # 2.async for 接受token(直接显示)\n",
        "    async for token in response.astream({\n",
        "        \"input\": query,\n",
        "        \"chat_history\": [],\n",
        "        \"agent_scratchpad\": []\n",
        "    }):\n",
        "        tokens.append(token)\n",
        "        print(token, flush=True)\n",
        "\n",
        "await stream(\"What is 10 + 10\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "流式处理主要拆分所有包含 JSON 参数的字段（arguments 和 args），而其他结构化字段要么保持不变，要么因为无法解析不完整 JSON 而变空。\n",
        "\n",
        "本质上：任何需要\"构建\"的内容都会被拆分，任何\"静态\"的元数据都保持不变。\n",
        "\n",
        "json_str = '{\"x\": 10, \"y\": 20}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2LLJ9ESKMmI",
        "outputId": "d1ad6937-5a44-4da9-8cb5-c7e3b3fdc265"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_4iifThkIlMBZUX5J2JSnG1m8', 'function': {'arguments': '{\"x\":10,\"y\":10}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-3abebb2f-5890-40b8-bcb3-2a400a30a751', tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': 'call_4iifThkIlMBZUX5J2JSnG1m8', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'add', 'args': '{\"x\":10,\"y\":10}', 'id': 'call_4iifThkIlMBZUX5J2JSnG1m8', 'index': 0, 'type': 'tool_call_chunk'}])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tk = tokens[0]\n",
        "\n",
        "for token in tokens[1:]:\n",
        "    tk += token\n",
        "\n",
        "tk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7upXtKmP4o9"
      },
      "source": [
        "Now we're seeing that the output is being streamed token-by-token. Because we're being streamed a tool call the `content` field is empty. Instead, we can see the tokens being added inside the `tool_calls` fields, within `id`, `function.name`, and `function.arguments`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# chunk = {\n",
        "#     'message': AIMessageChunk(                    # ← message 字段包含真正的消息\n",
        "#         content='',\n",
        "#         additional_kwargs={\n",
        "#             'tool_calls': [...]\n",
        "#         },\n",
        "#         response_metadata={...},\n",
        "#         id='run-xxx',\n",
        "#         tool_calls=[...],\n",
        "#         tool_call_chunks=[...]\n",
        "#     ),\n",
        "#     'generation_info': {                          # ← 生成相关信息\n",
        "#         'finish_reason': 'tool_calls',\n",
        "#         'model_name': 'gpt-4o-mini-2024-07-18',\n",
        "#         'system_fingerprint': 'fp_72ed7ab54c'\n",
        "#     }\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YtHO7IzdP4o9"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "class CustomAgentExecutor:\n",
        "    chat_history: list[BaseMessage]\n",
        "\n",
        "    def __init__(self, max_iterations: int = 3):\n",
        "        self.chat_history = []\n",
        "        self.max_iterations = max_iterations\n",
        "        self.agent: RunnableSerializable = (\n",
        "            {\n",
        "                \"input\": lambda x: x[\"input\"],\n",
        "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "            }\n",
        "            | prompt\n",
        "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
        "        )\n",
        "\n",
        "    # verbose = True, 显示详细信息(调试模式)\n",
        "    async def invoke(self, input: str, streamer: QueueCallbackHandler, verbose: bool = False) -> dict:\n",
        "        \n",
        "        # invoke the agent but we do this iteratively in a loop until\n",
        "        # reaching a final answer\n",
        "        count = 0\n",
        "        agent_scratchpad = []\n",
        "        while count < self.max_iterations:\n",
        "            # invoke a step for the agent to generate a tool call\n",
        "            async def stream(query: str):\n",
        "                response = self.agent.with_config(\n",
        "                    callbacks=[streamer]\n",
        "                )\n",
        "                # we initialize the output dictionary that we will be populating with\n",
        "                # our streamed output\n",
        "                output = None\n",
        "                # now we begin streaming\n",
        "                async for token in response.astream({\n",
        "                    \"input\": query,\n",
        "                    \"chat_history\": self.chat_history,\n",
        "                    \"agent_scratchpad\": agent_scratchpad\n",
        "                }):\n",
        "                    if output is None:\n",
        "                        output = token\n",
        "                    else:\n",
        "                        # we can just add the tokens together as they are streamed and\n",
        "                        # we'll have the full response object at the end\n",
        "                        output += token # 这里是AIMessageChunk对象的合并\n",
        "                    if token.content != \"\":\n",
        "                        # we can capture various parts of the response object\n",
        "                        if verbose: print(f\"content: {token.content}\", flush=True)\n",
        "                    tool_calls = token.additional_kwargs.get(\"tool_calls\")\n",
        "                    if tool_calls:\n",
        "                        if verbose: print(f\"tool_calls: {tool_calls}\", flush=True)\n",
        "                        tool_name = tool_calls[0][\"function\"][\"name\"]\n",
        "                        if tool_name:\n",
        "                            if verbose: print(f\"tool_name: {tool_name}\", flush=True)\n",
        "                        arg = tool_calls[0][\"function\"][\"arguments\"]\n",
        "                        if arg != \"\":\n",
        "                            if verbose: print(f\"arg: {arg}\", flush=True)\n",
        "                return AIMessage(\n",
        "                    content=output.content,\n",
        "                    tool_calls=output.tool_calls,\n",
        "                    tool_call_id=output.tool_calls[0][\"id\"]\n",
        "                )\n",
        "\n",
        "            tool_call = await stream(query=input)\n",
        "            # add initial tool call to scratchpad\n",
        "            agent_scratchpad.append(tool_call)\n",
        "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
        "            tool_name = tool_call.tool_calls[0][\"name\"]\n",
        "            tool_args = tool_call.tool_calls[0][\"args\"]\n",
        "            tool_call_id = tool_call.tool_call_id\n",
        "            tool_out = name2tool[tool_name](**tool_args)\n",
        "            # add the tool output to the agent scratchpad\n",
        "            tool_exec = ToolMessage(\n",
        "                content=f\"{tool_out}\",\n",
        "                tool_call_id=tool_call_id\n",
        "            )\n",
        "            agent_scratchpad.append(tool_exec)\n",
        "            count += 1\n",
        "            # if the tool call is the final answer tool, we stop\n",
        "            if tool_name == \"final_answer\":\n",
        "                break\n",
        "        # add the final output to the chat history, we only add the \"answer\" field\n",
        "        final_answer = tool_out[\"answer\"]\n",
        "        self.chat_history.extend([\n",
        "            HumanMessage(content=input),\n",
        "            AIMessage(content=final_answer)\n",
        "        ])\n",
        "        # return the final answer in dict form\n",
        "        return tool_args\n",
        "\n",
        "agent_executor = CustomAgentExecutor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0HKjOO5P4o9"
      },
      "source": [
        "We've added a few `print` statements to help us see what is being output, we activate those by setting `verbose=True`. Let's see what is returned:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDf6uLhPP4o9",
        "outputId": "c339416a-01da-43e1-bc22-d83f31c985ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tool_calls: [{'index': 0, 'id': 'call_EdkHIvqtJ1swTXaU2cHWCDoC', 'function': {'arguments': '', 'name': 'add'}, 'type': 'function'}]\n",
            "tool_name: add\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '{\"', 'name': None}, 'type': None}]\n",
            "arg: {\"\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': 'x', 'name': None}, 'type': None}]\n",
            "arg: x\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]\n",
            "arg: \":\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '10', 'name': None}, 'type': None}]\n",
            "arg: 10\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': ',\"', 'name': None}, 'type': None}]\n",
            "arg: ,\"\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': 'y', 'name': None}, 'type': None}]\n",
            "arg: y\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]\n",
            "arg: \":\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '10', 'name': None}, 'type': None}]\n",
            "arg: 10\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '}', 'name': None}, 'type': None}]\n",
            "arg: }\n",
            "tool_calls: [{'index': 0, 'id': 'call_LbqGPN7wvbvn00Xww5cJZ0It', 'function': {'arguments': '', 'name': 'final_answer'}, 'type': 'function'}]\n",
            "tool_name: final_answer\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '{\"', 'name': None}, 'type': None}]\n",
            "arg: {\"\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': 'answer', 'name': None}, 'type': None}]\n",
            "arg: answer\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '\":\"', 'name': None}, 'type': None}]\n",
            "arg: \":\"\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '10', 'name': None}, 'type': None}]\n",
            "arg: 10\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': ' +', 'name': None}, 'type': None}]\n",
            "arg:  +\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': ' ', 'name': None}, 'type': None}]\n",
            "arg:  \n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '10', 'name': None}, 'type': None}]\n",
            "arg: 10\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': ' equals', 'name': None}, 'type': None}]\n",
            "arg:  equals\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': ' ', 'name': None}, 'type': None}]\n",
            "arg:  \n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '20', 'name': None}, 'type': None}]\n",
            "arg: 20\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '.\",\"', 'name': None}, 'type': None}]\n",
            "arg: .\",\"\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': 'tools', 'name': None}, 'type': None}]\n",
            "arg: tools\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '_used', 'name': None}, 'type': None}]\n",
            "arg: _used\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '\":[\"', 'name': None}, 'type': None}]\n",
            "arg: \":[\"\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': 'functions', 'name': None}, 'type': None}]\n",
            "arg: functions\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '.add', 'name': None}, 'type': None}]\n",
            "arg: .add\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '\"]', 'name': None}, 'type': None}]\n",
            "arg: \"]\n",
            "tool_calls: [{'index': 0, 'id': None, 'function': {'arguments': '}', 'name': None}, 'type': None}]\n",
            "arg: }\n"
          ]
        }
      ],
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "out = await agent_executor.invoke(\"What is 10 + 10\", streamer, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK8HcGbeP4o-"
      },
      "source": [
        "We can see what is being output through the `verbose=True` flag. However, if we do _not_ `print` the output, we will see nothing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jQhdzX4YP4o-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'answer': '10 + 10 equals 20.', 'tools_used': ['functions.add']}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "out = await agent_executor.invoke(\"What is 10 + 10\", streamer)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='What is 10 + 10', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='10 + 10 equals 20.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What is 10 + 10', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='10 + 10 equals 20.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What is 10 + 10', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='10 + 10 equals 20.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What is 10 + 10', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='10 + 10 equals 20.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.chat_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV7-zSOuP4o-"
      },
      "source": [
        "Although we see nothing, it does not mean that nothing is being returned to us - we're just not using our callback handler and `asyncio.Queue`. To use these we create an `asyncio` task, iterate over the `__aiter__` method of our `streamer` object, and await the task, like so:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "时间线：\n",
        "t1: task = create_task(invoke(...))  ← 开始 invoke，立即返回\n",
        "t2: async for token in streamer:     ← 开始处理队列\n",
        "t3:     invoke 生成 token1          ← 并行：invoke 工作\n",
        "t4:     print(token1)               ← 并行：显示 token1\n",
        "t5:     invoke 生成 token2          ← 并行：invoke 工作\n",
        "t6:     print(token2)               ← 并行：显示 token2\n",
        "...\n",
        "tn: await task                      ← 等待 invoke 完全结束"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "create_task() 的作用就是：\n",
        "\n",
        "👉 启动一个异步任务，在后台执行，不阻塞当前流程。\n",
        "\n",
        "主线程就可以一边等它产出 token（通过 queue），一边实时处理这些 token。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_7popEQpetW0ZqEdFYU8Qwtoy', 'function': {'arguments': '', 'name': 'add'}, 'type': 'function'}]}, response_metadata={}, id='run-4483f4c2-ad4f-4ccb-b53d-a02d461df11d', tool_calls=[{'name': 'add', 'args': {}, 'id': 'call_7popEQpetW0ZqEdFYU8Qwtoy', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'add', 'args': '', 'id': 'call_7popEQpetW0ZqEdFYU8Qwtoy', 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '{\"', 'name': None}, 'type': None}]}, response_metadata={}, id='run-4483f4c2-ad4f-4ccb-b53d-a02d461df11d', tool_calls=[{'name': '', 'args': {}, 'id': None, 'type': 'tool_call'}], tool_call_chunks=[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'x', 'name': None}, 'type': None}]}, response_metadata={}, id='run-4483f4c2-ad4f-4ccb-b53d-a02d461df11d', invalid_tool_calls=[{'name': None, 'args': 'x', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'x', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]}, response_metadata={}, id='run-4483f4c2-ad4f-4ccb-b53d-a02d461df11d', invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '10', 'name': None}, 'type': None}]}, response_metadata={}, id='run-4483f4c2-ad4f-4ccb-b53d-a02d461df11d', invalid_tool_calls=[{'name': None, 'args': '10', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '10', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ',\"', 'name': None}, 'type': None}]}, response_metadata={}, id='run-4483f4c2-ad4f-4ccb-b53d-a02d461df11d', invalid_tool_calls=[{'name': None, 'args': ',\"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ',\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'y', 'name': None}, 'type': None}]}, response_metadata={}, id='run-4483f4c2-ad4f-4ccb-b53d-a02d461df11d', invalid_tool_calls=[{'name': None, 'args': 'y', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'y', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]}, response_metadata={}, id='run-4483f4c2-ad4f-4ccb-b53d-a02d461df11d', invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '10', 'name': None}, 'type': None}]}, response_metadata={}, id='run-4483f4c2-ad4f-4ccb-b53d-a02d461df11d', invalid_tool_calls=[{'name': None, 'args': '10', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '10', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '}', 'name': None}, 'type': None}]}, response_metadata={}, id='run-4483f4c2-ad4f-4ccb-b53d-a02d461df11d', invalid_tool_calls=[{'name': None, 'args': '}', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "generation_info={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'} message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, id='run-4483f4c2-ad4f-4ccb-b53d-a02d461df11d')\n",
            "<<STEP_END>>\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_3lAJgFnp5UPQs2oEpo235g09', 'function': {'arguments': '', 'name': 'final_answer'}, 'type': 'function'}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', tool_calls=[{'name': 'final_answer', 'args': {}, 'id': 'call_3lAJgFnp5UPQs2oEpo235g09', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'final_answer', 'args': '', 'id': 'call_3lAJgFnp5UPQs2oEpo235g09', 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '{\"', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', tool_calls=[{'name': '', 'args': {}, 'id': None, 'type': 'tool_call'}], tool_call_chunks=[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'answer', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': 'answer', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'answer', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":\"', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': '\":\"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\":\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '10', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': '10', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '10', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' +', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': ' +', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' +', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' ', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': ' ', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' ', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '10', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': '10', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '10', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' equals', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': ' equals', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' equals', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' ', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': ' ', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' ', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '20', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': '20', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '20', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '.\",\"', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': '.\",\"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '.\",\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'tools', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': 'tools', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'tools', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '_used', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': '_used', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '_used', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":[\"', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': '\":[\"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\":[\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'functions', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': 'functions', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'functions', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '.add', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': '.add', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '.add', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\"]', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': '\"]', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\"]', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '}', 'name': None}, 'type': None}]}, response_metadata={}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c', invalid_tool_calls=[{'name': None, 'args': '}', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "generation_info={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'} message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, id='run-8ea5bd8b-957e-4db6-80ec-1513cdcfba2c')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'answer': '10 + 10 equals 20.', 'tools_used': ['functions.add']}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "# create_task将一个异步函数包装成异步任务，让它可以并行执行\n",
        "# 这一步生产数据,不阻塞线程，任务和队列处理同时进行\n",
        "task = asyncio.create_task(agent_executor.invoke(\"What is 10 + 10\", streamer)) #立即开始执行但不用等待完成\n",
        "\n",
        "# 这一步消费数据，触发_aiter_\n",
        "# 这一步类比 async for token in streamer.__aiter__()\n",
        "async for token in streamer: #立即开始处理队列 \n",
        "    print(token, flush=True)\n",
        "\n",
        "# 最后等待任务完成\n",
        "await task\n",
        "\n",
        "\n",
        "## 这里task和async for token in streamer是并行的，"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr0-ZMqaP4o-"
      },
      "source": [
        "Although this seems like a lot of work, we're now streaming tokens in a way that allows us to pass these tokens on to other parts of our code - such as through a websocket, streamed API response, or some downstream processing.\n",
        "\n",
        "Let's try this out, we'll put together some simple post-processing to allow us to more nicely format the streamed output from out agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy-yvF62P4o-",
        "outputId": "01c133d6-a64c-4c6e-b613-fe28414b0dfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calling add...\n",
            "{\"｜x｜\":｜10｜,\"｜y｜\":｜10｜}｜\n",
            "\n",
            "Calling final_answer...\n",
            "{\"｜answer｜\":\"｜10｜ +｜ ｜10｜ equals｜ ｜20｜.\",\"｜tools｜_used｜\":[\"｜functions｜.add｜\"]｜}｜"
          ]
        }
      ],
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "task = asyncio.create_task(agent_executor.invoke(\"What is 10 + 10\", streamer))\n",
        "\n",
        "async for token in streamer:\n",
        "    # first identify if we have a <<STEP_END>> token\n",
        "    if token == \"<<STEP_END>>\":\n",
        "        print(\"\\n\", flush=True)\n",
        "    # we'll first identify if the token is a tool call\n",
        "    elif tool_calls := token.message.additional_kwargs.get(\"tool_calls\"):\n",
        "        # if we have a tool call with a tool name, we'll print it\n",
        "        if tool_name := tool_calls[0][\"function\"][\"name\"]:\n",
        "            print(f\"Calling {tool_name}...\", flush=True)\n",
        "        # if we have a tool call with arguments, we ad them to our args string\n",
        "        if tool_args := tool_calls[0][\"function\"][\"arguments\"]:\n",
        "            print(f\"{tool_args}\", end=\"｜\", flush=True)\n",
        "\n",
        "_ = await task #_表示我并不关心返回值"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1mv4tmEP4o-"
      },
      "source": [
        "With that we've produced a nice streaming output within our notebook - which ofcourse can be applied with very similar logic elsewhere, such as within a more polished web app."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## yield 的工作原理\n",
        "\n",
        "yield 确实会暂停函数执行，但不是暂停整个循环，而是：\n",
        "\n",
        "1. 返回一个值给调用者\n",
        "\n",
        "2. 暂停在 yield 这一行\n",
        "\n",
        "3. 等待调用者请求下一个值\n",
        "\n",
        "4. 从 yield 的位置继续执行\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始生成器\n",
            "收到: 第一个数据\n",
            "处理这个数据中...\n",
            "处理完毕\n",
            "第一个数据被处理完了，继续执行\n",
            "收到: 第二个数据\n",
            "处理这个数据中...\n",
            "处理完毕\n",
            "第二个数据被处理完了，继续执行\n",
            "生成器结束\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "async def simple_generator():  #这是异步生成器函数\n",
        "    print(\"开始生成器\")\n",
        "    \n",
        "    yield \"第一个数据\"  # ← 暂停在这里，返回数据\n",
        "    print(\"第一个数据被处理完了，继续执行\")\n",
        "    \n",
        "    yield \"第二个数据\"  # ← 暂停在这里，返回数据  \n",
        "    print(\"第二个数据被处理完了，继续执行\")\n",
        "    \n",
        "    print(\"生成器结束\")\n",
        "\n",
        "# 使用方式\n",
        "async def main():\n",
        "    async for item in simple_generator(): #simple_generator()是生成器对象\n",
        "        print(f\"收到: {item}\")\n",
        "        print(\"处理这个数据中...\")\n",
        "        await asyncio.sleep(3)  # 模拟处理时间\n",
        "        print(\"处理完毕\")\n",
        "\n",
        "#过程：\n",
        "# 1.执行到 yield，暂停，把值给 item\n",
        "# 2.外面 async for 拿到这个值，执行处理逻辑\n",
        "# 3.下一次自动 resume（恢复）到上一次暂停的地方\n",
        "\n",
        "# 输出顺序：\n",
        "# 开始生成器\n",
        "# 收到: 第一个数据\n",
        "# 处理这个数据中...\n",
        "# 处理完毕\n",
        "# 第一个数据被处理完了，继续执行\n",
        "# 收到: 第二个数据  \n",
        "# 处理这个数据中...\n",
        "# 处理完毕\n",
        "# 第二个数据被处理完了，继续执行\n",
        "# 生成器结束\n",
        "await main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 2, 3)\n",
            "{'name': 'Alice', 'age': 30}\n"
          ]
        }
      ],
      "source": [
        "def fn(*args, **kwargs):\n",
        "    print(args)    # 接收位置参数（变为元组）\n",
        "    print(kwargs)  # 接收关键字参数（变为字典）\n",
        "\n",
        "fn(1, 2, 3, name=\"Alice\", age=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "以下展示了create_task不await 和 await的区别。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 演示1: create_task 后不跟 await ===\n",
            "任务已创建，程序继续...\n",
            "主程序工作 1: 23:52:58\n",
            "任务 A 开始: 23:52:58\n",
            "主程序工作 2: 23:52:58\n",
            "主程序工作 3: 23:52:59\n",
            "主程序结束，但任务可能还没完成!\n",
            "\n",
            "=== 演示2: create_task 后跟 await ===\n",
            "任务已创建，程序继续...\n",
            "主程序工作 1: 23:52:59\n",
            "任务 B 开始: 23:52:59\n",
            "任务 A 完成: 23:53:00\n",
            "主程序工作 2: 23:53:00\n",
            "主程序工作 3: 23:53:00\n",
            "等待任务完成...\n",
            "任务 B 完成: 23:53:01\n",
            "任务完成，结果: 任务 B 的结果\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import time\n",
        "\n",
        "async def slow_task(name):\n",
        "    print(f\"任务 {name} 开始: {time.strftime('%H:%M:%S')}\")\n",
        "    await asyncio.sleep(2)\n",
        "    print(f\"任务 {name} 完成: {time.strftime('%H:%M:%S')}\")\n",
        "    return f\"任务 {name} 的结果\"\n",
        "\n",
        "async def demo_no_await():\n",
        "    print(\"=== 演示1: create_task 后不跟 await ===\")\n",
        "    \n",
        "    # 创建任务但不等待\n",
        "    task = asyncio.create_task(slow_task(\"A\"))\n",
        "    \n",
        "    print(\"任务已创建，程序继续...\")\n",
        "    \n",
        "    # 做一些其他工作\n",
        "    for i in range(3):\n",
        "        print(f\"主程序工作 {i+1}: {time.strftime('%H:%M:%S')}\")\n",
        "        await asyncio.sleep(0.5)\n",
        "    \n",
        "    print(\"主程序结束，但任务可能还没完成!\")\n",
        "    # 注意：这里没有 await task\n",
        "\n",
        "async def demo_with_await():\n",
        "    print(\"\\n=== 演示2: create_task 后跟 await ===\")\n",
        "    \n",
        "    # 创建任务并等待\n",
        "    task = asyncio.create_task(slow_task(\"B\"))\n",
        "    \n",
        "    print(\"任务已创建，程序继续...\")\n",
        "    \n",
        "    # 做一些其他工作\n",
        "    for i in range(3):\n",
        "        print(f\"主程序工作 {i+1}: {time.strftime('%H:%M:%S')}\")\n",
        "        await asyncio.sleep(0.5)\n",
        "    \n",
        "    print(\"等待任务完成...\")\n",
        "    result = await task\n",
        "    print(f\"任务完成，结果: {result}\")\n",
        "\n",
        "# 运行演示\n",
        "await (demo_no_await())\n",
        "await(demo_with_await())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
