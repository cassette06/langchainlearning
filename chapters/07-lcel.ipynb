{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AsMFKCALr3y"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/07-lcel.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PQji83qLpVC"
   },
   "source": [
    "#### LangChain Essentials Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22dLBWVZLpVD"
   },
   "source": [
    "# LangChains Expression Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiRUqmfBLpVE"
   },
   "source": [
    "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
    "\n",
    "In this example, we will introduce LangChain's Expression Langauge (LCEL), abstracting a full chain and understanding how it will work. We'll provide examples for both OpenAI's `gpt-4o-mini` *and* Meta's `llama3.2` via Ollama!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "i690tIabLwb_"
   },
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  langchain-core==0.3.33 \\\n",
    "  langchain-openai==0.3.3 \\\n",
    "  langchain-community==0.3.16 \\\n",
    "  langsmith==0.3.4 \\\n",
    "  docarray==0.40.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOR87lEpLpVE"
   },
   "source": [
    "---\n",
    "\n",
    "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfpD-R6sLpVE"
   },
   "source": [
    "---\n",
    "\n",
    "> ⚠️ If using LangSmith, add your API key below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "exBqQHgqLpVE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
    "    getpass(\"Enter LangSmith API Key: \")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-lcel-openai\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYGeByKjLpVF"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQbgErgdLpVF"
   },
   "source": [
    "## Traditional Chains vs LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dyluns_ELpVF"
   },
   "source": [
    "In this section we're going to dive into a basic example using the traditional method for building chains before jumping into LCEL. We will build a pipeline where the user must input a specific topic, and then the LLM will look and return a report on the specified topic. Generating a _research report_ for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTZTTiK7LpVF"
   },
   "source": [
    "### Traditional LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDHfbKZ7LpVF"
   },
   "source": [
    "The `LLMChain` is the simplest chain originally introduced in LangChain. This chain takes a prompt, feeds it into an LLM, and _optionally_ adds an output parsing step before returning the result.\n",
    "\n",
    "Let's see how we construct this using the traditional method, for this we need:\n",
    "\n",
    "* `prompt` — a `PromptTemplate` that will be used to generate the prompt for the LLM.\n",
    "* `llm` — the LLM we will be using to generate the output.\n",
    "* `output_parser` — an optional output parser that will be used to parse the structured output of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Jyz2vWLoLpVF"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"Give me a small report on {topic}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rykro39YLpVF"
   },
   "source": [
    "For the LLM, we'll start by initializing our connection to the OpenAI API. We do need an OpenAI API key, which you can get from the [OpenAI platform](https://platform.openai.com/api-keys).\n",
    "\n",
    "We will use the `gpt-4o-mini` model with a `temperature` of `0.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Uvs7hILRLpVF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
    "    or getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhs6_3c4LpVF",
    "outputId": "b7051222-b74e-45c0-d19f-ac28a6bb87fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-021dbdb0-8be2-4f36-ac9d-f669a8d39d70-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out = llm.invoke(\"Hello there\")\n",
    "llm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V39qjMpLLpVG"
   },
   "source": [
    "Then we define our output parser, this will be used to parse the output of the LLM. In this case, we will use the `StrOutputParser` which will parse the `AIMessage` output from our LLM into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "46qTjklnLpVG"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "z0uwxedBLpVG",
    "outputId": "faa83a90-aa81-40ba-b9a9-625507c7b7be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = output_parser.invoke(llm_out)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk4CUy52LpVG"
   },
   "source": [
    "Through the `LLMChain` class we can place each of our components into a linear `chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PrFk1J9dLpVG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7l/93h0p1b103v6bmb2440lc3740000gn/T/ipykernel_61505/2960353250.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjNsVMVpLpVG"
   },
   "source": [
    "Note that the `LLMChain` _was_ deprecated in LangChain `0.1.17`, the expected way of constructing these chains today is through LCEL, which we'll cover in a moment.\n",
    "\n",
    "We can `invoke` our `chain`, providing a `topic` that we'd like to be researched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDDq8Cq7LpVG",
    "outputId": "e30de2aa-7005-45f4-cb96-e681e9c2c1d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'retrieval augmented generation',\n",
       " 'text': '### Report on Retrieval-Augmented Generation (RAG)\\n\\n#### Introduction\\nRetrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge sources, thereby improving the accuracy and relevance of generated responses.\\n\\n#### Concept Overview\\nRAG operates on the principle of integrating a retrieval mechanism with a generative model. The process typically involves two main components:\\n\\n1. **Retrieval Component**: This part of the system retrieves relevant documents or pieces of information from a large corpus based on a given query. It uses techniques such as vector embeddings and similarity search to identify the most pertinent data.\\n\\n2. **Generation Component**: After retrieving the relevant information, the generative model (often based on architectures like Transformers) synthesizes a coherent and contextually appropriate response. This model leverages the retrieved data to enhance its output, ensuring that the generated text is not only fluent but also factually grounded.\\n\\n#### Advantages\\n- **Enhanced Knowledge Access**: RAG allows models to tap into vast external knowledge bases, making them more informed and capable of providing accurate information.\\n- **Improved Contextual Relevance**: By retrieving specific documents related to a query, the generated responses are more contextually relevant and tailored to user needs.\\n- **Dynamic Learning**: RAG systems can be updated with new information without retraining the entire model, allowing for continuous improvement and adaptation to new data.\\n\\n#### Applications\\nRAG has a wide range of applications, including:\\n- **Question Answering**: Providing precise answers to user queries by retrieving relevant documents and generating responses based on them.\\n- **Chatbots and Virtual Assistants**: Enhancing conversational agents with up-to-date information and contextually aware responses.\\n- **Content Creation**: Assisting in generating articles, summaries, or reports by pulling in relevant data from various sources.\\n\\n#### Challenges\\nDespite its advantages, RAG also faces several challenges:\\n- **Quality of Retrieved Information**: The effectiveness of the generation depends heavily on the quality and relevance of the retrieved documents.\\n- **Complexity of Integration**: Combining retrieval and generation components can introduce complexity in system design and implementation.\\n- **Computational Resources**: RAG systems may require significant computational resources, especially when dealing with large corpora for retrieval.\\n\\n#### Conclusion\\nRetrieval-Augmented Generation represents a significant advancement in the field of natural language processing. By effectively combining retrieval and generation, RAG systems can produce more accurate, relevant, and context-aware outputs. As research and development in this area continue to evolve, RAG is poised to play a crucial role in various applications, enhancing the capabilities of AI-driven systems in understanding and generating human language. \\n\\n#### Future Directions\\nFuture research may focus on improving the efficiency of retrieval mechanisms, enhancing the integration of diverse data sources, and developing methods to ensure the reliability and trustworthiness of the information retrieved. Additionally, exploring the ethical implications and biases in RAG systems will be essential for responsible AI deployment.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbHqc1qLLpVG"
   },
   "source": [
    "We can view a formatted version of this output using the `Markdown` display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 980
    },
    "id": "hBxPHOZ-LpVG",
    "outputId": "23c54f43-dc98-4c56-a47f-4a996b206eb7"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Report on Retrieval-Augmented Generation (RAG)\n",
       "\n",
       "#### Introduction\n",
       "Retrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge sources, thereby improving the accuracy and relevance of generated responses.\n",
       "\n",
       "#### Concept Overview\n",
       "RAG operates on the principle of integrating a retrieval mechanism with a generative model. The process typically involves two main components:\n",
       "\n",
       "1. **Retrieval Component**: This part of the system retrieves relevant documents or pieces of information from a large corpus based on a given query. It uses techniques such as vector embeddings and similarity search to identify the most pertinent data.\n",
       "\n",
       "2. **Generation Component**: After retrieving the relevant information, the generative model (often based on architectures like Transformers) synthesizes a coherent and contextually appropriate response. This model leverages the retrieved data to enhance its output, ensuring that the generated text is not only fluent but also factually grounded.\n",
       "\n",
       "#### Advantages\n",
       "- **Enhanced Knowledge Access**: RAG allows models to tap into vast external knowledge bases, making them more informed and capable of providing accurate information.\n",
       "- **Improved Contextual Relevance**: By retrieving specific documents related to a query, the generated responses are more contextually relevant and tailored to user needs.\n",
       "- **Dynamic Learning**: RAG systems can be updated with new information without retraining the entire model, allowing for continuous improvement and adaptation to new data.\n",
       "\n",
       "#### Applications\n",
       "RAG has a wide range of applications, including:\n",
       "- **Question Answering**: Providing precise answers to user queries by retrieving relevant documents and generating responses based on them.\n",
       "- **Chatbots and Virtual Assistants**: Enhancing conversational agents with up-to-date information and contextually aware responses.\n",
       "- **Content Creation**: Assisting in generating articles, summaries, or reports by pulling in relevant data from various sources.\n",
       "\n",
       "#### Challenges\n",
       "Despite its advantages, RAG also faces several challenges:\n",
       "- **Quality of Retrieved Information**: The effectiveness of the generation depends heavily on the quality and relevance of the retrieved documents.\n",
       "- **Complexity of Integration**: Combining retrieval and generation components can introduce complexity in system design and implementation.\n",
       "- **Computational Resources**: RAG systems may require significant computational resources, especially when dealing with large corpora for retrieval.\n",
       "\n",
       "#### Conclusion\n",
       "Retrieval-Augmented Generation represents a significant advancement in the field of natural language processing. By effectively combining retrieval and generation, RAG systems can produce more accurate, relevant, and context-aware outputs. As research and development in this area continue to evolve, RAG is poised to play a crucial role in various applications, enhancing the capabilities of AI-driven systems in understanding and generating human language. \n",
       "\n",
       "#### Future Directions\n",
       "Future research may focus on improving the efficiency of retrieval mechanisms, enhancing the integration of diverse data sources, and developing methods to ensure the reliability and trustworthiness of the information retrieved. Additionally, exploring the ethical implications and biases in RAG systems will be essential for responsible AI deployment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(result[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbmq0DfeLpVG"
   },
   "source": [
    "That is a simple `LLMChain` using the traditional LangChain method. Now let's move onto LCEL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWets8OpLpVG"
   },
   "source": [
    "## LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Lr5NA0xLpVG"
   },
   "source": [
    "**L**ang**C**hain **E**xpression **L**anguage (LCEL) is the recommended approach to building chains in LangChain. Having superceeded the traditional methods with `LLMChain`, etc. LCEL gives us a more flexible system for building chains. The pipe operator `|` is used by LCEL to _chain_ together components. Let's see how we'd construct an `LLMChain` using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "8R-9b7ulLpVH"
   },
   "outputs": [],
   "source": [
    "lcel_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdJvzetzLpVH"
   },
   "source": [
    "We can `invoke` this chain in the same way as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "XKWzIwpCLpVH",
    "outputId": "9dc83fdd-98a4-4513-dcac-055092aeeb12"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"### Report on Retrieval-Augmented Generation (RAG)\\n\\n#### Introduction\\nRetrieval-Augmented Generation (RAG) is an advanced approach in natural language processing (NLP) that combines the strengths of information retrieval and generative models. This technique enhances the capabilities of language models by allowing them to access external knowledge bases or documents during the generation process, thereby improving the relevance and accuracy of the generated content.\\n\\n#### Key Components\\n1. **Retrieval Mechanism**: RAG employs a retrieval system to fetch relevant documents or pieces of information from a large corpus based on the input query. This is typically done using techniques such as vector embeddings or traditional keyword matching.\\n\\n2. **Generative Model**: After retrieving relevant information, a generative model (often based on transformer architectures like BERT or GPT) processes this information along with the original query to produce coherent and contextually appropriate responses.\\n\\n3. **Integration**: The integration of retrieval and generation can occur in various ways, such as:\\n   - **End-to-End Training**: The retrieval and generation components are trained together, allowing the model to learn how to select the most relevant information for generating responses.\\n   - **Pipeline Approach**: The retrieval and generation processes are handled separately, where the retrieval system first fetches relevant documents, and the generative model then uses these documents to create responses.\\n\\n#### Advantages\\n- **Enhanced Knowledge Access**: RAG allows models to leverage vast external knowledge, making them more informed and capable of answering a wider range of queries.\\n- **Improved Accuracy**: By grounding responses in retrieved documents, RAG can reduce hallucinations (the generation of incorrect or nonsensical information) that are common in standalone generative models.\\n- **Dynamic Updates**: The retrieval component can be updated independently of the generative model, allowing the system to incorporate new information without retraining the entire model.\\n\\n#### Applications\\n- **Question Answering**: RAG is particularly effective in systems designed for question answering, where users seek specific information that may not be contained within the model's training data.\\n- **Chatbots and Virtual Assistants**: By providing contextually relevant information, RAG can enhance the conversational abilities of chatbots, making interactions more informative and engaging.\\n- **Content Creation**: RAG can assist in generating articles, summaries, or reports by retrieving relevant data and synthesizing it into coherent narratives.\\n\\n#### Challenges\\n- **Retrieval Quality**: The effectiveness of RAG heavily depends on the quality of the retrieval system. Poor retrieval can lead to irrelevant or misleading information being used in the generation process.\\n- **Computational Complexity**: The dual nature of retrieval and generation can increase the computational resources required, making it less efficient than simpler models in some scenarios.\\n- **Integration Complexity**: Balancing the interaction between the retrieval and generation components can be complex, requiring careful tuning and optimization.\\n\\n#### Conclusion\\nRetrieval-Augmented Generation represents a significant advancement in the field of NLP, combining the strengths of information retrieval and generative modeling to create more accurate and contextually relevant outputs. As research and development in this area continue, RAG is likely to play a crucial role in the evolution of intelligent systems capable of understanding and generating human-like text.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7E8_msHLpVH"
   },
   "source": [
    "The output format is slightly different, but the underlying functionality and content being output is the same. As before, we can view a formatted version of this output using the `Markdown` display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "id": "42uPzB1JLpVH",
    "outputId": "0d7254e1-0cfe-43ff-dea2-dfad2f760007"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Report on Retrieval-Augmented Generation (RAG)\n",
       "\n",
       "#### Introduction\n",
       "Retrieval-Augmented Generation (RAG) is an advanced approach in natural language processing (NLP) that combines the strengths of information retrieval and generative models. This technique enhances the capabilities of language models by allowing them to access external knowledge bases or documents during the generation process, thereby improving the relevance and accuracy of the generated content.\n",
       "\n",
       "#### Key Components\n",
       "1. **Retrieval Mechanism**: RAG employs a retrieval system to fetch relevant documents or pieces of information from a large corpus based on the input query. This is typically done using techniques such as vector embeddings or traditional keyword matching.\n",
       "\n",
       "2. **Generative Model**: After retrieving relevant information, a generative model (often based on transformer architectures like BERT or GPT) processes this information along with the original query to produce coherent and contextually appropriate responses.\n",
       "\n",
       "3. **Integration**: The integration of retrieval and generation can occur in various ways, such as:\n",
       "   - **End-to-End Training**: The retrieval and generation components are trained together, allowing the model to learn how to select the most relevant information for generating responses.\n",
       "   - **Pipeline Approach**: The retrieval and generation processes are handled separately, where the retrieval system first fetches relevant documents, and the generative model then uses these documents to create responses.\n",
       "\n",
       "#### Advantages\n",
       "- **Enhanced Knowledge Access**: RAG allows models to leverage vast external knowledge, making them more informed and capable of answering a wider range of queries.\n",
       "- **Improved Accuracy**: By grounding responses in retrieved documents, RAG can reduce hallucinations (the generation of incorrect or nonsensical information) that are common in standalone generative models.\n",
       "- **Dynamic Updates**: The retrieval component can be updated independently of the generative model, allowing the system to incorporate new information without retraining the entire model.\n",
       "\n",
       "#### Applications\n",
       "- **Question Answering**: RAG is particularly effective in systems designed for question answering, where users seek specific information that may not be contained within the model's training data.\n",
       "- **Chatbots and Virtual Assistants**: By providing contextually relevant information, RAG can enhance the conversational abilities of chatbots, making interactions more informative and engaging.\n",
       "- **Content Creation**: RAG can assist in generating articles, summaries, or reports by retrieving relevant data and synthesizing it into coherent narratives.\n",
       "\n",
       "#### Challenges\n",
       "- **Retrieval Quality**: The effectiveness of RAG heavily depends on the quality of the retrieval system. Poor retrieval can lead to irrelevant or misleading information being used in the generation process.\n",
       "- **Computational Complexity**: The dual nature of retrieval and generation can increase the computational resources required, making it less efficient than simpler models in some scenarios.\n",
       "- **Integration Complexity**: Balancing the interaction between the retrieval and generation components can be complex, requiring careful tuning and optimization.\n",
       "\n",
       "#### Conclusion\n",
       "Retrieval-Augmented Generation represents a significant advancement in the field of NLP, combining the strengths of information retrieval and generative modeling to create more accurate and contextually relevant outputs. As research and development in this area continue, RAG is likely to play a crucial role in the evolution of intelligent systems capable of understanding and generating human-like text."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbutFcpyLpVI"
   },
   "source": [
    "### How Does the Pipe Operator Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIjOY7VILpVI"
   },
   "source": [
    "Before moving onto other LCEL features, let's take a moment to understand what the pipe operator `|` is doing and _how_ it works.\n",
    "\n",
    "Functionality wise, the pipe tells you that whatever the _left_ side outputs will be fed as input into the _right_ side. In the example of `prompt | llm | output_parser`, we see that `prompt` feeds into `llm` feeds into `output_parser`.\n",
    "\n",
    "The pipe operator is a way of chaining together components, and is a way of saying that whatever the _left_ side outputs will be fed as input into the _right_ side.\n",
    "\n",
    "Let's make a basic class named `Runnable` that will transform our a provided function into a _runnable_ class that we will then use with the pipe `|` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "9xTvwvF4LpVI"
   },
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other.invoke(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2Vg_PhXLpVI"
   },
   "source": [
    "With the `Runnable` class, we will be able wrap a function into the class, allowing us to then chain together multiple of these _runnable_ functions using the `__or__` method.\n",
    "\n",
    "First, let's create a few functions that we'll chain together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "G7HCw9-VLpVI"
   },
   "outputs": [],
   "source": [
    "def add_five(x):\n",
    "    return x+5\n",
    "\n",
    "def sub_five(x):\n",
    "    return x-5\n",
    "\n",
    "def mul_five(x):\n",
    "    return x*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0fDvmtWLpVI"
   },
   "source": [
    "Now we wrap our functions with the `Runnable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "LfnBBcbxLpVI"
   },
   "outputs": [],
   "source": [
    "add_five_runnable = Runnable(add_five)\n",
    "sub_five_runnable = Runnable(sub_five)\n",
    "mul_five_runnable = Runnable(mul_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jHLbl_7LpVI"
   },
   "source": [
    "Finally, we can chain these together using the `__or__` method from the `Runnable` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHFo61TwLpVI",
    "outputId": "3a03c07b-7ca9-45f5-ab2e-a979565b7e42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
    "\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oim1zWHLpVI"
   },
   "source": [
    "So we can see that we're able to chain together our functions using `__or__`. The pipe `|` operator is simply a shortcut for the `__or__` method, so we can create the exact same chain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uh3-jF8pLpVJ",
    "outputId": "933ea922-f587-4f98-e825-b26113c5dc08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable\n",
    "\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Lnj00T8LpVJ"
   },
   "source": [
    "## LCEL `RunnableLambda`\n",
    "\n",
    "The `RunnableLambda` class is LangChain's built-in method for constructing a _runnable_ object from a function. That is, it does the same thing as the custom `Runnable` class we created earlier. Let's try it out with the same functions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "BmMH8GzVLpVJ"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "add_five_runnable = RunnableLambda(add_five)\n",
    "sub_five_runnable = RunnableLambda(sub_five)\n",
    "mul_five_runnable = RunnableLambda(mul_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkpwIKq6LpVJ"
   },
   "source": [
    "We chain these together again with the pipe `|` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "9e58vaSELpVJ"
   },
   "outputs": [],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FdqbdAyLpVJ"
   },
   "source": [
    "And call them using the `invoke` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqHBMT8ULpVJ",
    "outputId": "b32dcaf1-1207-4cd4-f881-d9ffd5a754d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47mu4xxqLpVJ"
   },
   "source": [
    "Now we want to try something a little more testing, so this time we will generate a report, and we will try and edit that report using this functionallity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "xRJLzshqLpVJ"
   },
   "outputs": [],
   "source": [
    "prompt_str = \"give me a small report about {topic}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "lfl_s4VALpVJ"
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WSMlRM8wLpVJ",
    "outputId": "2b25fd0b-8e2c-4513-9980-25f587eee58b"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Report on Artificial Intelligence (AI)\n",
       "\n",
       "#### Introduction\n",
       "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think and learn like humans. It encompasses a variety of technologies and methodologies, including machine learning, natural language processing, robotics, and computer vision. AI has rapidly evolved over the past few decades, becoming an integral part of various industries and daily life.\n",
       "\n",
       "#### Historical Context\n",
       "The concept of AI dates back to the mid-20th century, with pioneers like Alan Turing and John McCarthy laying the groundwork. The term \"artificial intelligence\" was coined in 1956 during the Dartmouth Conference, which is considered the birth of AI as a field. Early AI research focused on problem-solving and symbolic methods, but progress was slow due to limited computational power and data.\n",
       "\n",
       "#### Current Trends\n",
       "1. **Machine Learning (ML)**: A subset of AI that enables systems to learn from data and improve over time without explicit programming. Deep learning, a branch of ML, uses neural networks to analyze vast amounts of data, leading to breakthroughs in image and speech recognition.\n",
       "\n",
       "2. **Natural Language Processing (NLP)**: This technology allows machines to understand and respond to human language. Applications include chatbots, virtual assistants, and language translation services.\n",
       "\n",
       "3. **Robotics**: AI is increasingly integrated into robotics, enabling machines to perform complex tasks in manufacturing, healthcare, and logistics. Autonomous vehicles are a prominent example of AI in robotics.\n",
       "\n",
       "4. **Ethics and Regulation**: As AI technologies advance, ethical considerations and regulatory frameworks are becoming critical. Issues such as bias in algorithms, data privacy, and the impact of automation on jobs are at the forefront of discussions among policymakers and technologists.\n",
       "\n",
       "#### Applications\n",
       "AI is transforming various sectors, including:\n",
       "\n",
       "- **Healthcare**: AI algorithms assist in diagnosing diseases, personalizing treatment plans, and managing patient data.\n",
       "- **Finance**: AI is used for fraud detection, algorithmic trading, and customer service through chatbots.\n",
       "- **Retail**: Personalized shopping experiences, inventory management, and supply chain optimization are enhanced by AI.\n",
       "- **Entertainment**: Streaming services use AI to recommend content based on user preferences.\n",
       "\n",
       "#### Challenges\n",
       "Despite its potential, AI faces several challenges:\n",
       "\n",
       "- **Data Privacy**: The collection and use of personal data raise concerns about privacy and security.\n",
       "- **Bias and Fairness**: AI systems can perpetuate existing biases if trained on biased data, leading to unfair outcomes.\n",
       "- **Job Displacement**: Automation may lead to job losses in certain sectors, necessitating workforce retraining and adaptation.\n",
       "\n",
       "#### Conclusion\n",
       "Artificial Intelligence is a rapidly evolving field with the potential to revolutionize industries and improve quality of life. However, it also presents significant challenges that require careful consideration and proactive management. As AI continues to advance, collaboration among technologists, ethicists, and policymakers will be essential to harness its benefits while mitigating risks.\n",
       "\n",
       "#### Recommendations\n",
       "- **Invest in Education**: Promote AI literacy and training programs to prepare the workforce for an AI-driven economy.\n",
       "- **Establish Ethical Guidelines**: Develop frameworks to ensure the responsible use of AI technologies.\n",
       "- **Encourage Collaboration**: Foster partnerships between academia, industry, and government to address challenges and drive innovation.\n",
       "\n",
       "---\n",
       "\n",
       "This report provides a brief overview of AI, its current trends, applications, challenges, and recommendations for future development."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"AI\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XE7HOxwaLpVJ"
   },
   "source": [
    "Here we are making two functions, `extract_fact` to pull out the main content of our text and `replace_word` that will replace AI with Skynet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Wc7ws8FkLpVJ"
   },
   "outputs": [],
   "source": [
    "# \\n 表示换行符\n",
    "# \\n\\n 表示空行\n",
    "def extract_fact(x):\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "old_word = \"AI\"\n",
    "new_word = \"skynet\"\n",
    "\n",
    "def replace_word(x):\n",
    "    return x.replace(old_word, new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我想开发AI agent, \n",
      "你想开发吗？\n",
      "\n",
      "你好，\n",
      "我的名字是lxy\n",
      "\n",
      "我想开发skynet agent, \n",
      "\n",
      "你想开发吗？\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好，\\n我的名字是lxy\\n我想开发AI agent, \\n你想开发吗？\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"\"\"你好，\n",
    "我的名字是lxy\n",
    "\n",
    "我想开发AI agent, \n",
    "\n",
    "你想开发吗？\n",
    "\"\"\"\n",
    "print(extract_fact(x))\n",
    "print(replace_word(x))\n",
    "\n",
    "\"\\n\".join(x.split(\"\\n\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktHrg5iSLpVK"
   },
   "source": [
    "Lets wrap these functions and see what the output is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "_vFeqMY-LpVK"
   },
   "outputs": [],
   "source": [
    "extract_fact_runnable = RunnableLambda(extract_fact)\n",
    "replace_word_runnable = RunnableLambda(replace_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "TDWN-tQzLpVK"
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser | extract_fact_runnable | replace_word_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 950
    },
    "id": "OCODP8vLLpVK",
    "outputId": "7d4c8a62-1b61-42fa-e6c3-f0dbe634f7f5"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Introduction\n",
       "Retrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge sources, thereby improving the accuracy and relevance of generated responses.\n",
       "#### Concept Overview\n",
       "RAG operates on the principle of integrating a retrieval mechanism with a generative model. The process typically involves two main components:\n",
       "1. **Retrieval Component**: This part of the system searches a large corpus of documents or knowledge bases to find relevant information based on a given query. It employs techniques such as vector embeddings and similarity search to identify the most pertinent documents.\n",
       "2. **Generation Component**: Once relevant documents are retrieved, the generative model (often based on architectures like Transformers) synthesizes a coherent response by incorporating the retrieved information. This allows the model to produce more informed and contextually appropriate outputs.\n",
       "#### Advantages\n",
       "- **Enhanced Knowledge Access**: RAG allows models to leverage vast external datasets, which can significantly improve the quality of responses, especially in domains requiring up-to-date or specialized knowledge.\n",
       "- **Reduced Hallucination**: Traditional generative models sometimes produce inaccurate or fabricated information (a phenomenon known as \"hallucination\"). By grounding responses in retrieved documents, RAG can mitigate this issue.\n",
       "- **Dynamic Adaptability**: The retrieval component can be updated independently of the generative model, allowing the system to adapt to new information without retraining the entire model.\n",
       "#### Applications\n",
       "RAG has a wide range of applications, including:\n",
       "- **Question Answering**: Providing accurate answers to user queries by retrieving relevant documents and generating responses based on that information.\n",
       "- **Chatbots and Virtual Assistants**: Enhancing conversational agents with the ability to pull in real-time data and provide contextually relevant answers.\n",
       "- **Content Creation**: Assisting in generating articles, reports, or summaries by retrieving and synthesizing information from multiple sources.\n",
       "#### Challenges\n",
       "Despite its advantages, RAG faces several challenges:\n",
       "- **Complexity**: The integration of retrieval and generation components can complicate the system architecture and increase computational requirements.\n",
       "- **Quality of Retrieved Information**: The effectiveness of RAG heavily depends on the quality and relevance of the retrieved documents. Poor retrieval can lead to suboptimal generation.\n",
       "- **Latency**: The retrieval process can introduce delays, which may affect user experience in real-time applications.\n",
       "#### Conclusion\n",
       "Retrieval-Augmented Generation represents a significant advancement in the field of natural language processing, combining the strengths of retrieval and generation to produce more accurate and contextually relevant outputs. As research and development in this area continue, RAG is poised to play a crucial role in various applications, enhancing the capabilities of skynet systems in understanding and generating human-like text. Future work will likely focus on improving retrieval efficiency, enhancing the quality of generated content, and addressing the challenges associated with system complexity and latency."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHSumR1tLpVK"
   },
   "source": [
    "Those are our `RunnableLambda` functions. It's worth noting that all inputs to these functions are expected to be a SINGLE arguments. If you have a function that accepts multiple arguments, you can input a dictionary with keys, then unpack them inside the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableLambda 是 LangChain 中用来包装普通 Python 函数的类，让它们可以在 LCEL 管道中使用。 \\n\n",
    "它能包装的函数只能有一个参数。如果函数要传多个参数，可以使用字典传参，在函数内部解包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始函数（多参数）\n",
    "def calculate_total(price, tax_rate, discount):\n",
    "    total = price * (1 + tax_rate) * (1 - discount)\n",
    "    return round(total, 2)\n",
    "\n",
    "# 普通调用\n",
    "result = calculate_total(100, 0.1, 0.2)  # 88.0\n",
    "# 内部解包\n",
    "def calculate_total_unpack(data):\n",
    "    # 解包字典\n",
    "    return calculate_total(**data)\n",
    "\n",
    "calc_runnable = RunnableLambda(calculate_total_unpack)\n",
    "\n",
    "# 使用\n",
    "result = calc_runnable.invoke({\n",
    "    'price': 100,\n",
    "    'tax_rate': 0.1,\n",
    "    'discount': 0.2\n",
    "})\n",
    "print(result)  # 88.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIXKLyQKLpVK"
   },
   "source": [
    "## LCEL `RunnableParallel` and `RunnablePassthrough`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avFhxg_pLpVK"
   },
   "source": [
    "LCEL provides us with various `Runnable` classes that allow us to control the flow of data and execution order through our chains. Two of these are `RunnableParallel` and `RunnablePassthrough`.\n",
    "\n",
    "* `RunnableParallel` — allows us to run multiple `Runnable` instances in parallel. Acting almost as a Y-fork in the chain.\n",
    "\n",
    "* `RunnablePassthrough` — allows us to pass through a variable to the next `Runnable` without modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTsqIAufLpVK"
   },
   "source": [
    "To see these runnables in action, we will create two data sources, each source provides specific information but to answer the question we will need both to fed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zaD678FMLpVK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7l/93h0p1b103v6bmb2440lc3740000gn/T/ipykernel_61505/3699332964.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings()\n",
      "/Users/liuxinyi/lxy_learning/langchain_learning/langchain-course/.venv/lib/python3.12/site-packages/docarray/helper.py:224: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  \"\"\"\n",
      "/Users/liuxinyi/lxy_learning/langchain_learning/langchain-course/.venv/lib/python3.12/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "# 创建嵌入模型： OpenAIEmbeddings()用来将文本转换为向量表示\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# 创建两个向量数据库\n",
    "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"half the info is here\",\n",
    "        \"DeepSeek-V3 was released in December 2024\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")\n",
    "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"the other half of the info is here\",\n",
    "        \"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ub4pjRrBLpVK"
   },
   "source": [
    "Here you can see the prompt does have three inputs, two for context and one for the question itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnxXrw-CLpVK"
   },
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Using the context provided, answer the user's question.\n",
    "Context:\n",
    "{context_a}\n",
    "{context_b}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "jL3YN1c1LpVK"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt_str),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YchbLoKLpVK"
   },
   "source": [
    "Here we are wrapping our vector stores as retrievers so they can be fitted into one big retrieval variable to be used by the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnablePassthrough() 就像一个\"原样传递\"的管道，它的作用是：接收到什么，就输出什么，不做任何处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "xkhFV8-SLpVL"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# 将向量存储（VectorStore） 转换为 检索器（Retriever）\n",
    "# 向量存储主要用于存储和管理向量数据\n",
    "# 检索器专门用于根据查询检索相关文档\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\": retriever_a, \n",
    "        \"context_b\": retriever_b, \n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKtORU5xLpVL"
   },
   "source": [
    "The chain we'll be constructing will look something like this:\n",
    "\n",
    "![](https://github.com/aurelio-labs/langchain-course/blob/main/assets/lcel-flow.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "PNLpfS3_LpVL"
   },
   "outputs": [],
   "source": [
    "chain = retrieval | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo8BfprxLpVL"
   },
   "source": [
    "We `invoke` it as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "84gEXUG1LpVL",
    "outputId": "432d6af2-b59c-4dc5-d390-1ecbfc3efbd1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The DeepSeek-V3 model, released in December 2024, uses a mixture of experts architecture with 671 billion parameters.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\n",
    "    \"what architecture does the model DeepSeek released in december use?\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcwr16jsLpVL"
   },
   "source": [
    "With that we've seen how we can use `RunnableParallel` and `RunnablePassthrough` to control the flow of data and execution order through our chains.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是 Runnable？ \\\\\\\n",
    "Runnable 是 LangChain 中的一个核心概念，它是一个标准化的接口，任何实现了这个接口的对象都可以：\n",
    "\n",
    "\n",
    "被调用：有 invoke() 方法\n",
    "\n",
    "\n",
    "被链接：支持 | 操作符\n",
    "\n",
    "\n",
    "被组合：可以与其他 Runnable 连接\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一种写法\n",
    "# 但输入数据格式已经完全匹配prompt期望的格式时，可以直接使用prompt\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# prompt定义 期望的格式\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"give me a small report about {topic}\"\n",
    ")\n",
    "\n",
    "# 直接调用，输入格式匹配\n",
    "chain.invoke({\"topic\": \"AI\"})  # ✅ 直接匹配\n",
    "# 或者\n",
    "chain.invoke(\"AI\")  # ✅ 如果只有一个变量，可能支持直接传字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设你的输入是这样的复杂格式\n",
    "input_data = {\n",
    "    \"user_query\": \"What is RAG?\",\n",
    "    \"retrieved_docs\": [\"doc1\", \"doc2\"],\n",
    "    \"metadata\": {...}\n",
    "}\n",
    "\n",
    "# 但prompt期望的是这样的格式\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"Based on context: {context}, answer: {query}\"\n",
    ")\n",
    "\n",
    "# 所以需要先转换\n",
    "pipeline = (\n",
    "    {\n",
    "        \"query\": lambda x: x[\"user_query\"],      # 从user_query提取到query\n",
    "        \"context\": lambda x: x[\"retrieved_docs\"] # 从retrieved_docs提取到context\n",
    "    }\n",
    "    | prompt_template  # 现在格式匹配了\n",
    "    | llm\n",
    ")\n",
    "\n",
    "result = pipeline.invoke(input_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
